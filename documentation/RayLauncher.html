<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.10.0" />
<title>RayLauncher API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>RayLauncher</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">from typing import Any, Callable, List
import subprocess
import sys
import time
import os
import dill
import paramiko
from getpass import getpass
import re

dill.settings[&#34;recurse&#34;] = True


class RayLauncher:
    &#34;&#34;&#34;A class that automatically connects RAY workers and executes the function requested by the user&#34;&#34;&#34;

    def __init__(
        self,
        project_name: str = None,
        func: Callable = None,
        args: dict = None,
        files: List[str] = [],
        modules: List[str] = [],
        node_nbr: int = 1,
        use_gpu: bool = False,
        memory: int = 64,
        max_running_time: int = 60,
        runtime_env: dict = {&#34;env_vars&#34;: {}},
        server_run: bool = True,
        server_ssh: str = &#34;curnagl.dcsr.unil.ch&#34;,
        server_username: str = &#34;hjamet&#34;,
        server_password: str = None,
    ):
        &#34;&#34;&#34;Initialize the launcher

        Args:
            project_name (str, optional): Name of the project. Defaults to None.
            func (Callable, optional): Function to execute. This function should not be remote but can use ray ressources. Defaults to None.
            args (dict, optional): Arguments of the function. Defaults to None.
            files (List[str], optional): List of files to push to the cluster. This path must be **relative** to the project directory. Defaults to [].
            modules (List[str], optional): List of modules to load on the curnagl Cluster. Use `module spider` to see available modules. Defaults to None.
            node_nbr (int, optional): Number of nodes to use. Defaults to 1.
            use_gpu (bool, optional): Use GPU or not. Defaults to False.
            memory (int, optional): Amount of RAM to use per node in GigaBytes. Defaults to 64.
            max_running_time (int, optional): Maximum running time of the job in minutes. Defaults to 60.
            runtime_env (dict, optional): Environment variables to share between all the workers. Can be useful for issues like https://github.com/ray-project/ray/issues/418. Default to empty.
            server_run (bool, optional): If you run the launcher from your local machine, you can use this parameter to execute your function using online cluster ressources. Defaults to True.
            server_ssh (str, optional): If `server_run` is set to true, the addess of the **SLURM** server to use.
            server_username (str, optional): If `server_run` is set to true, the username with which you wish to connect.
            server_password (str, optional): If `server_run` is set to true, the password of the user to connect to the server. CAUTION: never write your password in the code. Defaults to None.
        &#34;&#34;&#34;
        # Save the parameters
        self.project_name = project_name
        self.func = func
        self.args = args
        self.files = files
        self.node_nbr = node_nbr
        self.use_gpu = use_gpu
        self.memory = memory
        self.max_running_time = max_running_time
        self.runtime_env = runtime_env
        self.server_run = server_run
        self.server_ssh = server_ssh
        self.server_username = server_username
        self.server_password = server_password

        self.modules = [&#34;gcc&#34;, &#34;python/3.9.13&#34;] + [
            mod for mod in modules if mod not in [&#34;gcc&#34;, &#34;python/3.9.13&#34;]
        ]
        if self.use_gpu is True and &#34;cuda&#34; not in self.modules:
            self.modules += [&#34;cuda&#34;, &#34;cudnn&#34;]

        # Check if this code is running on a cluster
        self.cluster = os.path.exists(&#34;/usr/bin/sbatch&#34;)

        # Create the project directory if not exists
        self.module_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), &#34;..&#34;
        )
        self.pwd_path = os.getcwd()
        self.module_path = os.path.dirname(os.path.abspath(__file__))
        self.project_path = os.path.join(self.pwd_path, &#34;.slogs&#34;, self.project_name)
        if not os.path.exists(self.project_path):
            os.makedirs(self.project_path)

        if not self.server_run:
            self.__write_python_script()
            self.script_file, self.job_name = self.__write_slurm_script()

    def __call__(self, cancel_old_jobs: bool = True) -&gt; Any:
        &#34;&#34;&#34;Launch the job and return the result

        Args:
            cancel_old_jobs (bool, optional): Cancel the old jobs. Defaults to True.

        Returns:
            Any: Result of the function
        &#34;&#34;&#34;
        # Sereialize function and arguments
        self.__serialize_func_and_args(self.func, self.args)

        if self.cluster:
            print(&#34;Cluster detected, running on cluster...&#34;)
            # Cancel the old jobs
            if cancel_old_jobs:
                print(&#34;Canceling old jobs...&#34;)
                subprocess.Popen(
                    [&#34;scancel&#34;, &#34;-u&#34;, os.environ[&#34;USER&#34;]],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                )
            # Launch the job
            self.__launch_job(self.script_file, self.job_name)
        elif self.server_run:
            self.__launch_server(cancel_old_jobs)
        else:
            print(&#34;No cluster detected, running locally...&#34;)
            subprocess.Popen(
                [sys.executable, os.path.join(self.project_path, &#34;spython.py&#34;)]
            )

        # Load the result
        while not os.path.exists(os.path.join(self.project_path, &#34;result.pkl&#34;)):
            time.sleep(0.25)
        with open(os.path.join(self.project_path, &#34;result.pkl&#34;), &#34;rb&#34;) as f:
            result = dill.load(f)

        return result

    def __push_file(
        self, file_path: str, sftp: paramiko.SFTPClient, ssh_client: paramiko.SSHClient
    ):
        &#34;&#34;&#34;Push a file to the cluster

        Args:
            file_path (str): Path to the file to push. This path must be **relative** to the project directory.
        &#34;&#34;&#34;
        print(f&#34;Pushing file {os.path.basename(file_path)} to the cluster...&#34;)

        # Determine the path to the file
        local_path = file_path
        local_path_from_pwd = os.path.relpath(local_path, self.pwd_path)
        cluster_path = os.path.join(
            &#34;/users&#34;, self.server_username, &#34;slurmray-server&#34;, local_path_from_pwd
        )

        # Create the directory if not exists

        stdin, stdout, stderr = ssh_client.exec_command(
            f&#34;mkdir -p &#39;{os.path.dirname(cluster_path)}&#39;&#34;
        )
        while True:
            line = stdout.readline()
            if not line:
                break
            print(line, end=&#34;&#34;)
        time.sleep(1)  # Wait for the directory to be created

        # Copy the file to the server
        sftp.put(file_path, cluster_path)

    def __serialize_func_and_args(self, func: Callable = None, args: list = None):
        &#34;&#34;&#34;Serialize the function and the arguments

        Args:
            func (Callable, optional): Function to serialize. Defaults to None.
            args (list, optional): Arguments of the function. Defaults to None.
        &#34;&#34;&#34;
        print(&#34;Serializing function and arguments...&#34;)

        # Remove the old python script
        for file in os.listdir(self.project_path):
            if file.endswith(&#34;.pkl&#34;):
                os.remove(os.path.join(self.project_path, file))

        # Pickle the function
        with open(os.path.join(self.project_path, &#34;func.pkl&#34;), &#34;wb&#34;) as f:
            dill.dump(func, f)

        # Pickle the arguments
        if args is None:
            args = {}
        with open(os.path.join(self.project_path, &#34;args.pkl&#34;), &#34;wb&#34;) as f:
            dill.dump(args, f)

    def __write_python_script(self):
        &#34;&#34;&#34;Write the python script that will be executed by the job&#34;&#34;&#34;
        print(&#34;Writing python script...&#34;)

        # Remove the old python script
        for file in os.listdir(self.project_path):
            if file.endswith(&#34;.py&#34;):
                os.remove(os.path.join(self.project_path, file))

        # Write the python script
        with open(
            os.path.join(self.module_path, &#34;assets&#34;, &#34;spython_template.py&#34;),
            &#34;r&#34;,
        ) as f:
            text = f.read()

        text = text.replace(&#34;{{PROJECT_PATH}}&#34;, f&#39;&#34;{self.project_path}&#34;&#39;)
        local_mode = &#34;&#34;
        if self.cluster or self.server_run:
            f&#34;\n\taddress=&#39;auto&#39;,\n\tinclude_dashboard=True,\n\tdashboard_host=&#39;0.0.0.0&#39;,\n\tdashboard_port=8888,\nruntime_env = {self.runtime_env},\n&#34;
        text = text.replace(
            &#34;{{LOCAL_MODE}}&#34;,
            local_mode,
        )
        with open(os.path.join(self.project_path, &#34;spython.py&#34;), &#34;w&#34;) as f:
            f.write(text)

    def __write_slurm_script(
        self,
    ):
        &#34;&#34;&#34;Write the slurm script that will be executed by the job

        Returns:
            str: Name of the script file
            str: Name of the job
        &#34;&#34;&#34;
        print(&#34;Writing slurm script...&#34;)
        template_file = os.path.join(self.module_path, &#34;assets&#34;, &#34;sbatch_template.sh&#34;)

        JOB_NAME = &#34;{{JOB_NAME}}&#34;
        NUM_NODES = &#34;{{NUM_NODES}}&#34;
        MEMORY = &#34;{{MEMORY}}&#34;
        RUNNING_TIME = &#34;{{RUNNING_TIME}}&#34;
        PARTITION_NAME = &#34;{{PARTITION_NAME}}&#34;
        COMMAND_PLACEHOLDER = &#34;{{COMMAND_PLACEHOLDER}}&#34;
        GIVEN_NODE = &#34;{{GIVEN_NODE}}&#34;
        COMMAND_SUFFIX = &#34;{{COMMAND_SUFFIX}}&#34;
        LOAD_ENV = &#34;{{LOAD_ENV}}&#34;
        PARTITION_SPECIFICS = &#34;{{PARTITION_SPECIFICS}}&#34;

        job_name = &#34;{}_{}&#34;.format(
            self.project_name, time.strftime(&#34;%d%m-%Hh%M&#34;, time.localtime())
        )

        # Convert the time to xx:xx:xx format
        max_time = &#34;{}:{}:{}&#34;.format(
            str(self.max_running_time // 60).zfill(2),
            str(self.max_running_time % 60).zfill(2),
            str(0).zfill(2),
        )

        # ===== Modified the template script =====
        with open(template_file, &#34;r&#34;) as f:
            text = f.read()
        text = text.replace(JOB_NAME, os.path.join(self.project_path, job_name))
        text = text.replace(NUM_NODES, str(self.node_nbr))
        text = text.replace(MEMORY, str(self.memory))
        text = text.replace(RUNNING_TIME, str(max_time))
        text = text.replace(PARTITION_NAME, str(&#34;gpu&#34; if self.use_gpu &gt; 0 else &#34;cpu&#34;))
        text = text.replace(
            COMMAND_PLACEHOLDER, str(f&#34;{sys.executable} {self.project_path}/spython.py&#34;)
        )
        text = text.replace(LOAD_ENV, str(f&#34;module load {&#39; &#39;.join(self.modules)}&#34;))
        text = text.replace(GIVEN_NODE, &#34;&#34;)
        text = text.replace(COMMAND_SUFFIX, &#34;&#34;)
        text = text.replace(
            &#34;# THIS FILE IS A TEMPLATE AND IT SHOULD NOT BE DEPLOYED TO &#34; &#34;PRODUCTION!&#34;,
            &#34;# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE &#34;
            &#34;RUNNABLE!&#34;,
        )

        # ===== Add partition specifics =====
        if self.use_gpu &gt; 0:
            text = text.replace(
                PARTITION_SPECIFICS,
                str(&#34;#SBATCH --gres gpu:1\n#SBATCH --gres-flags enforce-binding&#34;),
            )
        else:
            text = text.replace(PARTITION_SPECIFICS, &#34;#SBATCH --exclusive&#34;)

        # ===== Save the script =====
        script_file = &#34;sbatch.sh&#34;
        with open(os.path.join(self.project_path, script_file), &#34;w&#34;) as f:
            f.write(text)

        return script_file, job_name

    def __launch_job(self, script_file: str = None, job_name: str = None):
        &#34;&#34;&#34;Launch the job

        Args:
            script_file (str, optional): Name of the script file. Defaults to None.
            job_name (str, optional): Name of the job. Defaults to None.
        &#34;&#34;&#34;
        # ===== Submit the job =====
        print(&#34;Start to submit job!&#34;)
        subprocess.Popen([&#34;sbatch&#34;, os.path.join(self.project_path, script_file)])
        print(
            &#34;Job submitted! Script file is at: &lt;{}&gt;. Log file is at: &lt;{}&gt;&#34;.format(
                os.path.join(self.project_path, script_file),
                os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name)),
            )
        )

        # Wait for log file to be created
        current_queue = None
        queue_log_file = os.path.join(
            self.project_path, &#34;{}_queue.log&#34;.format(job_name)
        )
        with open(queue_log_file, &#34;w&#34;) as f:
            f.write(&#34;&#34;)
        print(
            &#34;Start to monitor the queue... You can check the queue at: &lt;{}&gt;&#34;.format(
                queue_log_file
            )
        )
        subprocess.Popen(
            [&#34;tail&#34;, &#34;-f&#34;, os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name))]
        )
        while True:
            time.sleep(0.25)
            if os.path.exists(
                os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name))
            ):
                break
            else:
                # Get result from squeue -p {{PARTITION_NAME}}
                result = subprocess.run(
                    [&#34;squeue&#34;, &#34;-p&#34;, &#34;gpu&#34; if self.use_gpu is True else &#34;cpu&#34;],
                    capture_output=True,
                )
                df = result.stdout.decode(&#34;utf-8&#34;).split(&#34;\n&#34;)
                users = list(
                    map(
                        lambda row: row[: len(df[0].split(&#34;ST&#34;)[0])][:-1].split(&#34; &#34;)[
                            -1
                        ],
                        df,
                    )
                )
                status = list(
                    map(
                        lambda row: row[len(df[0].split(&#34;ST&#34;)[0]) :]
                        .strip()
                        .split(&#34; &#34;)[0],
                        df,
                    )
                )
                nodes = list(
                    map(
                        lambda row: row[len(df[0].split(&#34;NODE&#34;)[0]) :]
                        .strip()
                        .split(&#34; &#34;)[0],
                        df,
                    )
                )
                node_list = list(
                    map(lambda row: row[len(df[0].split(&#34;NODELIST(REASON)&#34;)[0]) :], df)
                )

                to_queue = list(
                    zip(
                        users,
                        status,
                        nodes,
                        node_list,
                    )
                )[1:]
                if current_queue is None or current_queue != to_queue:
                    current_queue = to_queue
                    with open(queue_log_file, &#34;w&#34;) as f:
                        text = &#34;Current queue:\n&#34;
                        format_row = &#34;{:&gt;30}&#34; * (len(current_queue[0]))
                        for user, status, nodes, node_list in current_queue:
                            text += (
                                format_row.format(user, status, nodes, node_list) + &#34;\n&#34;
                            )
                        text += &#34;\n&#34;
                        f.write(text)

        # Wait for the job to finish while printing the log
        log_cursor_position = 0
        job_finished = False
        while not job_finished:
            time.sleep(0.25)
            if os.path.exists(os.path.join(self.project_path, &#34;result.pkl&#34;)):
                job_finished = True
            else:
                with open(
                    os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name)), &#34;r&#34;
                ) as f:
                    f.seek(log_cursor_position)
                    text = f.read()
                    if text != &#34;&#34;:
                        print(text, end=&#34;&#34;)
                    log_cursor_position = f.tell()

        print(&#34;Job finished!&#34;)

    def __launch_server(self, cancel_old_jobs: bool = True):
        &#34;&#34;&#34;Launch the server on the cluster and run the function using the ressources.

        Args:
            cancel_old_jobs (bool, optional): Whether or not to interrupt all the user&#39;s jobs on the cluster.
        &#34;&#34;&#34;
        connected = False
        print(&#34;Connecting to the cluster...&#34;)
        ssh_client = paramiko.SSHClient()
        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        while not connected:
            try:
                if self.server_password is None:
                    # Add ssh key
                    self.server_password = getpass(&#34;Enter your cluster password: &#34;)
                
                ssh_client.connect(
                    hostname=self.server_ssh,
                    username=self.server_username,
                    password=self.server_password,
                )
                sftp = ssh_client.open_sftp()
                connected = True
            except paramiko.ssh_exception.AuthenticationException:
                self.server_password = None
                print(&#34;Wrong password, please try again.&#34;)

        # Write server script
        self.__write_server_script()

        print(&#34;Downloading server...&#34;)
        # Generate requirements.txt
        subprocess.run(
            [f&#34;pip-chill --no-version &gt; {self.project_path}/requirements.txt&#34;],
            shell=True,
        )

        with open(f&#34;{self.project_path}/requirements.txt&#34;, &#34;r&#34;) as file:
            lines = file.readlines()
            # Adapt dependencies for the cluster
            # Adapt torch version (not needed anymore)
            # lines = [re.sub(r&#34;torch\n&#34;, &#34;torch==2.0.1\n&#34;, line) for line in lines]
            # lines = [re.sub(r&#39;torchvision\n&#39;, &#39;torchvision --pre --index-url https://download.pytorch.org/whl/nightly/cu121\n&#39;, line) for line in lines]
            # lines = [re.sub(r&#39;torchaudio\n&#39;, &#39;torchaudio --pre --index-url https://download.pytorch.org/whl/nightly/cu121\n&#39;, line) for line in lines]

            # lines = [re.sub(r&#39;bitsandbytes\n&#39;, &#39;bitsandbytes --global-option=&#34;--cuda_ext&#34;\n&#39;, line) for line in lines]
            lines = [re.sub(r&#34;slurmray\n&#34;, &#34;&#34;, line) for line in lines]
            # Add slurmray --pre
            lines.append(&#34;slurmray --pre\n&#34;)
            # Solve torch buf (https://github.com/pytorch/pytorch/issues/111469)
            if &#34;torchaudio\n&#34; or &#34;torchvision\n&#34; in lines:
                lines.append(&#34;torch==2.1.1\n&#34;)
                lines.append(&#34;--index-url https://download.pytorch.org/whl/cu121\n&#34;)

        with open(f&#34;{self.project_path}/requirements.txt&#34;, &#34;w&#34;) as file:
            file.writelines(lines)

        # Copy files from the project to the server
        for file in os.listdir(self.project_path):
            if file.endswith(&#34;.py&#34;) or file.endswith(&#34;.pkl&#34;) or file.endswith(&#34;.sh&#34;):
                sftp.put(os.path.join(self.project_path, file), file)

        # Create the server directory and remove old files
        ssh_client.exec_command(
            &#34;mkdir -p slurmray-server/.slogs/server &amp;&amp; rm -rf slurmray-server/.slogs/server/*&#34;
        )
        # Copy user files to the server
        for file in self.files:
            self.__push_file(file, sftp, ssh_client)
        # Copy the requirements.txt to the server
        sftp.put(
            os.path.join(self.project_path, &#34;requirements.txt&#34;), &#34;requirements.txt&#34;
        )
        # Copy the server script to the server
        sftp.put(
            os.path.join(self.module_path, &#34;assets&#34;, &#34;slurmray_server.sh&#34;),
            &#34;slurmray_server.sh&#34;,
        )
        # Chmod script
        sftp.chmod(&#34;slurmray_server.sh&#34;, 0o755)

        # Run the server
        print(&#34;Running server...&#34;)
        stdin, stdout, stderr = ssh_client.exec_command(&#34;./slurmray_server.sh&#34;)

        # Read the output in real time
        while True:
            line = stdout.readline()
            if not line:
                break
            print(line, end=&#34;&#34;)

        # Downloading result
        print(&#34;Downloading result...&#34;)
        sftp.get(
            &#34;slurmray-server/.slogs/server/result.pkl&#34;,
            os.path.join(self.project_path, &#34;result.pkl&#34;),
        )
        print(&#34;Result downloaded!&#34;)

    def __write_server_script(self):
        &#34;&#34;&#34;This funtion will write a script with the given specifications to run slurmray on the cluster&#34;&#34;&#34;
        print(&#34;Writing slurmray server script...&#34;)
        template_file = os.path.join(
            self.module_path, &#34;assets&#34;, &#34;slurmray_server_template.py&#34;
        )

        MODULES = self.modules
        NODE_NBR = self.node_nbr
        USE_GPU = self.use_gpu
        MEMORY = self.memory
        MAX_RUNNING_TIME = self.max_running_time

        # ===== Modified the template script =====
        with open(template_file, &#34;r&#34;) as f:
            text = f.read()
        text = text.replace(&#34;{{MODULES}}&#34;, str(MODULES))
        text = text.replace(&#34;{{NODE_NBR}}&#34;, str(NODE_NBR))
        text = text.replace(&#34;{{USE_GPU}}&#34;, str(USE_GPU))
        text = text.replace(&#34;{{MEMORY}}&#34;, str(MEMORY))
        text = text.replace(&#34;{{MAX_RUNNING_TIME}}&#34;, str(MAX_RUNNING_TIME))

        # ===== Save the script =====
        script_file = &#34;slurmray_server.py&#34;
        with open(os.path.join(self.project_path, script_file), &#34;w&#34;) as f:
            f.write(text)


# ---------------------------------------------------------------------------- #
#                             EXAMPLE OF EXECUTION                             #
# ---------------------------------------------------------------------------- #
if __name__ == &#34;__main__&#34;:
    import ray
    import torch

    def function_inside_function():
        with open(&#34;slurmray/RayLauncher.py&#34;, &#34;r&#34;) as f:
            return f.read()[0:10]

    def example_func(x):
        result = (
            ray.cluster_resources(),
            f&#34;GPU is available : {torch.cuda.is_available()}&#34;,
            x + 1,
            function_inside_function(),
        )
        return result

    launcher = RayLauncher(
        project_name=&#34;example&#34;, # Name of the project (will create a directory with this name in the current directory)
        func=example_func, # Function to execute
        args={&#34;x&#34;: 1}, # Arguments of the function
        files=[&#34;slurmray/RayLauncher.py&#34;], # List of files to push to the cluster (file path will be recreated on the cluster)
        modules=[], # List of modules to load on the curnagl Cluster (CUDA &amp; CUDNN are automatically added if use_gpu=True)
        node_nbr=1, # Number of nodes to use
        use_gpu=True, # If you need A100 GPU, you can set it to True
        memory=8, # In MegaBytes
        max_running_time=5, # In minutes
        runtime_env={&#34;env_vars&#34;: {&#34;NCCL_SOCKET_IFNAME&#34;: &#34;eno1&#34;}}, # Example of environment variable
        server_run=True, # To run the code on the cluster and not locally
        server_ssh=&#34;curnagl.dcsr.unil.ch&#34;, # Address of the SLURM server
        server_username=&#34;hjamet&#34;, # Username to connect to the server
        server_password=None, # Will be asked in the terminal
    )

    result = launcher()
    print(result)</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="RayLauncher.RayLauncher"><code class="flex name class">
<span>class <span class="ident">RayLauncher</span></span>
<span>(</span><span>project_name: str = None, func: Callable = None, args: dict = None, files: List[str] = [], modules: List[str] = [], node_nbr: int = 1, use_gpu: bool = False, memory: int = 64, max_running_time: int = 60, runtime_env: dict = {'env_vars': {}}, server_run: bool = True, server_ssh: str = 'curnagl.dcsr.unil.ch', server_username: str = 'hjamet', server_password: str = None)</span>
</code></dt>
<dd>
<div class="desc"><p>A class that automatically connects RAY workers and executes the function requested by the user</p>
<p>Initialize the launcher</p>
<h2 id="args">Args</h2>
<dl>
<dt><strong><code>project_name</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>Name of the project. Defaults to None.</dd>
<dt><strong><code>func</code></strong> :&ensp;<code>Callable</code>, optional</dt>
<dd>Function to execute. This function should not be remote but can use ray ressources. Defaults to None.</dd>
<dt><strong><code>args</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Arguments of the function. Defaults to None.</dd>
<dt><strong><code>files</code></strong> :&ensp;<code>List[str]</code>, optional</dt>
<dd>List of files to push to the cluster. This path must be <strong>relative</strong> to the project directory. Defaults to [].</dd>
<dt><strong><code>modules</code></strong> :&ensp;<code>List[str]</code>, optional</dt>
<dd>List of modules to load on the curnagl Cluster. Use <code>module spider</code> to see available modules. Defaults to None.</dd>
<dt><strong><code>node_nbr</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Number of nodes to use. Defaults to 1.</dd>
<dt><strong><code>use_gpu</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>Use GPU or not. Defaults to False.</dd>
<dt><strong><code>memory</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Amount of RAM to use per node in GigaBytes. Defaults to 64.</dd>
<dt><strong><code>max_running_time</code></strong> :&ensp;<code>int</code>, optional</dt>
<dd>Maximum running time of the job in minutes. Defaults to 60.</dd>
<dt><strong><code>runtime_env</code></strong> :&ensp;<code>dict</code>, optional</dt>
<dd>Environment variables to share between all the workers. Can be useful for issues like <a href="https://github.com/ray-project/ray/issues/418.">https://github.com/ray-project/ray/issues/418.</a> Default to empty.</dd>
<dt><strong><code>server_run</code></strong> :&ensp;<code>bool</code>, optional</dt>
<dd>If you run the launcher from your local machine, you can use this parameter to execute your function using online cluster ressources. Defaults to True.</dd>
<dt><strong><code>server_ssh</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If <code>server_run</code> is set to true, the addess of the <strong>SLURM</strong> server to use.</dd>
<dt><strong><code>server_username</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If <code>server_run</code> is set to true, the username with which you wish to connect.</dd>
<dt><strong><code>server_password</code></strong> :&ensp;<code>str</code>, optional</dt>
<dd>If <code>server_run</code> is set to true, the password of the user to connect to the server. CAUTION: never write your password in the code. Defaults to None.</dd>
</dl></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class RayLauncher:
    &#34;&#34;&#34;A class that automatically connects RAY workers and executes the function requested by the user&#34;&#34;&#34;

    def __init__(
        self,
        project_name: str = None,
        func: Callable = None,
        args: dict = None,
        files: List[str] = [],
        modules: List[str] = [],
        node_nbr: int = 1,
        use_gpu: bool = False,
        memory: int = 64,
        max_running_time: int = 60,
        runtime_env: dict = {&#34;env_vars&#34;: {}},
        server_run: bool = True,
        server_ssh: str = &#34;curnagl.dcsr.unil.ch&#34;,
        server_username: str = &#34;hjamet&#34;,
        server_password: str = None,
    ):
        &#34;&#34;&#34;Initialize the launcher

        Args:
            project_name (str, optional): Name of the project. Defaults to None.
            func (Callable, optional): Function to execute. This function should not be remote but can use ray ressources. Defaults to None.
            args (dict, optional): Arguments of the function. Defaults to None.
            files (List[str], optional): List of files to push to the cluster. This path must be **relative** to the project directory. Defaults to [].
            modules (List[str], optional): List of modules to load on the curnagl Cluster. Use `module spider` to see available modules. Defaults to None.
            node_nbr (int, optional): Number of nodes to use. Defaults to 1.
            use_gpu (bool, optional): Use GPU or not. Defaults to False.
            memory (int, optional): Amount of RAM to use per node in GigaBytes. Defaults to 64.
            max_running_time (int, optional): Maximum running time of the job in minutes. Defaults to 60.
            runtime_env (dict, optional): Environment variables to share between all the workers. Can be useful for issues like https://github.com/ray-project/ray/issues/418. Default to empty.
            server_run (bool, optional): If you run the launcher from your local machine, you can use this parameter to execute your function using online cluster ressources. Defaults to True.
            server_ssh (str, optional): If `server_run` is set to true, the addess of the **SLURM** server to use.
            server_username (str, optional): If `server_run` is set to true, the username with which you wish to connect.
            server_password (str, optional): If `server_run` is set to true, the password of the user to connect to the server. CAUTION: never write your password in the code. Defaults to None.
        &#34;&#34;&#34;
        # Save the parameters
        self.project_name = project_name
        self.func = func
        self.args = args
        self.files = files
        self.node_nbr = node_nbr
        self.use_gpu = use_gpu
        self.memory = memory
        self.max_running_time = max_running_time
        self.runtime_env = runtime_env
        self.server_run = server_run
        self.server_ssh = server_ssh
        self.server_username = server_username
        self.server_password = server_password

        self.modules = [&#34;gcc&#34;, &#34;python/3.9.13&#34;] + [
            mod for mod in modules if mod not in [&#34;gcc&#34;, &#34;python/3.9.13&#34;]
        ]
        if self.use_gpu is True and &#34;cuda&#34; not in self.modules:
            self.modules += [&#34;cuda&#34;, &#34;cudnn&#34;]

        # Check if this code is running on a cluster
        self.cluster = os.path.exists(&#34;/usr/bin/sbatch&#34;)

        # Create the project directory if not exists
        self.module_path = os.path.join(
            os.path.dirname(os.path.abspath(__file__)), &#34;..&#34;
        )
        self.pwd_path = os.getcwd()
        self.module_path = os.path.dirname(os.path.abspath(__file__))
        self.project_path = os.path.join(self.pwd_path, &#34;.slogs&#34;, self.project_name)
        if not os.path.exists(self.project_path):
            os.makedirs(self.project_path)

        if not self.server_run:
            self.__write_python_script()
            self.script_file, self.job_name = self.__write_slurm_script()

    def __call__(self, cancel_old_jobs: bool = True) -&gt; Any:
        &#34;&#34;&#34;Launch the job and return the result

        Args:
            cancel_old_jobs (bool, optional): Cancel the old jobs. Defaults to True.

        Returns:
            Any: Result of the function
        &#34;&#34;&#34;
        # Sereialize function and arguments
        self.__serialize_func_and_args(self.func, self.args)

        if self.cluster:
            print(&#34;Cluster detected, running on cluster...&#34;)
            # Cancel the old jobs
            if cancel_old_jobs:
                print(&#34;Canceling old jobs...&#34;)
                subprocess.Popen(
                    [&#34;scancel&#34;, &#34;-u&#34;, os.environ[&#34;USER&#34;]],
                    stdout=subprocess.PIPE,
                    stderr=subprocess.PIPE,
                )
            # Launch the job
            self.__launch_job(self.script_file, self.job_name)
        elif self.server_run:
            self.__launch_server(cancel_old_jobs)
        else:
            print(&#34;No cluster detected, running locally...&#34;)
            subprocess.Popen(
                [sys.executable, os.path.join(self.project_path, &#34;spython.py&#34;)]
            )

        # Load the result
        while not os.path.exists(os.path.join(self.project_path, &#34;result.pkl&#34;)):
            time.sleep(0.25)
        with open(os.path.join(self.project_path, &#34;result.pkl&#34;), &#34;rb&#34;) as f:
            result = dill.load(f)

        return result

    def __push_file(
        self, file_path: str, sftp: paramiko.SFTPClient, ssh_client: paramiko.SSHClient
    ):
        &#34;&#34;&#34;Push a file to the cluster

        Args:
            file_path (str): Path to the file to push. This path must be **relative** to the project directory.
        &#34;&#34;&#34;
        print(f&#34;Pushing file {os.path.basename(file_path)} to the cluster...&#34;)

        # Determine the path to the file
        local_path = file_path
        local_path_from_pwd = os.path.relpath(local_path, self.pwd_path)
        cluster_path = os.path.join(
            &#34;/users&#34;, self.server_username, &#34;slurmray-server&#34;, local_path_from_pwd
        )

        # Create the directory if not exists

        stdin, stdout, stderr = ssh_client.exec_command(
            f&#34;mkdir -p &#39;{os.path.dirname(cluster_path)}&#39;&#34;
        )
        while True:
            line = stdout.readline()
            if not line:
                break
            print(line, end=&#34;&#34;)
        time.sleep(1)  # Wait for the directory to be created

        # Copy the file to the server
        sftp.put(file_path, cluster_path)

    def __serialize_func_and_args(self, func: Callable = None, args: list = None):
        &#34;&#34;&#34;Serialize the function and the arguments

        Args:
            func (Callable, optional): Function to serialize. Defaults to None.
            args (list, optional): Arguments of the function. Defaults to None.
        &#34;&#34;&#34;
        print(&#34;Serializing function and arguments...&#34;)

        # Remove the old python script
        for file in os.listdir(self.project_path):
            if file.endswith(&#34;.pkl&#34;):
                os.remove(os.path.join(self.project_path, file))

        # Pickle the function
        with open(os.path.join(self.project_path, &#34;func.pkl&#34;), &#34;wb&#34;) as f:
            dill.dump(func, f)

        # Pickle the arguments
        if args is None:
            args = {}
        with open(os.path.join(self.project_path, &#34;args.pkl&#34;), &#34;wb&#34;) as f:
            dill.dump(args, f)

    def __write_python_script(self):
        &#34;&#34;&#34;Write the python script that will be executed by the job&#34;&#34;&#34;
        print(&#34;Writing python script...&#34;)

        # Remove the old python script
        for file in os.listdir(self.project_path):
            if file.endswith(&#34;.py&#34;):
                os.remove(os.path.join(self.project_path, file))

        # Write the python script
        with open(
            os.path.join(self.module_path, &#34;assets&#34;, &#34;spython_template.py&#34;),
            &#34;r&#34;,
        ) as f:
            text = f.read()

        text = text.replace(&#34;{{PROJECT_PATH}}&#34;, f&#39;&#34;{self.project_path}&#34;&#39;)
        local_mode = &#34;&#34;
        if self.cluster or self.server_run:
            f&#34;\n\taddress=&#39;auto&#39;,\n\tinclude_dashboard=True,\n\tdashboard_host=&#39;0.0.0.0&#39;,\n\tdashboard_port=8888,\nruntime_env = {self.runtime_env},\n&#34;
        text = text.replace(
            &#34;{{LOCAL_MODE}}&#34;,
            local_mode,
        )
        with open(os.path.join(self.project_path, &#34;spython.py&#34;), &#34;w&#34;) as f:
            f.write(text)

    def __write_slurm_script(
        self,
    ):
        &#34;&#34;&#34;Write the slurm script that will be executed by the job

        Returns:
            str: Name of the script file
            str: Name of the job
        &#34;&#34;&#34;
        print(&#34;Writing slurm script...&#34;)
        template_file = os.path.join(self.module_path, &#34;assets&#34;, &#34;sbatch_template.sh&#34;)

        JOB_NAME = &#34;{{JOB_NAME}}&#34;
        NUM_NODES = &#34;{{NUM_NODES}}&#34;
        MEMORY = &#34;{{MEMORY}}&#34;
        RUNNING_TIME = &#34;{{RUNNING_TIME}}&#34;
        PARTITION_NAME = &#34;{{PARTITION_NAME}}&#34;
        COMMAND_PLACEHOLDER = &#34;{{COMMAND_PLACEHOLDER}}&#34;
        GIVEN_NODE = &#34;{{GIVEN_NODE}}&#34;
        COMMAND_SUFFIX = &#34;{{COMMAND_SUFFIX}}&#34;
        LOAD_ENV = &#34;{{LOAD_ENV}}&#34;
        PARTITION_SPECIFICS = &#34;{{PARTITION_SPECIFICS}}&#34;

        job_name = &#34;{}_{}&#34;.format(
            self.project_name, time.strftime(&#34;%d%m-%Hh%M&#34;, time.localtime())
        )

        # Convert the time to xx:xx:xx format
        max_time = &#34;{}:{}:{}&#34;.format(
            str(self.max_running_time // 60).zfill(2),
            str(self.max_running_time % 60).zfill(2),
            str(0).zfill(2),
        )

        # ===== Modified the template script =====
        with open(template_file, &#34;r&#34;) as f:
            text = f.read()
        text = text.replace(JOB_NAME, os.path.join(self.project_path, job_name))
        text = text.replace(NUM_NODES, str(self.node_nbr))
        text = text.replace(MEMORY, str(self.memory))
        text = text.replace(RUNNING_TIME, str(max_time))
        text = text.replace(PARTITION_NAME, str(&#34;gpu&#34; if self.use_gpu &gt; 0 else &#34;cpu&#34;))
        text = text.replace(
            COMMAND_PLACEHOLDER, str(f&#34;{sys.executable} {self.project_path}/spython.py&#34;)
        )
        text = text.replace(LOAD_ENV, str(f&#34;module load {&#39; &#39;.join(self.modules)}&#34;))
        text = text.replace(GIVEN_NODE, &#34;&#34;)
        text = text.replace(COMMAND_SUFFIX, &#34;&#34;)
        text = text.replace(
            &#34;# THIS FILE IS A TEMPLATE AND IT SHOULD NOT BE DEPLOYED TO &#34; &#34;PRODUCTION!&#34;,
            &#34;# THIS FILE IS MODIFIED AUTOMATICALLY FROM TEMPLATE AND SHOULD BE &#34;
            &#34;RUNNABLE!&#34;,
        )

        # ===== Add partition specifics =====
        if self.use_gpu &gt; 0:
            text = text.replace(
                PARTITION_SPECIFICS,
                str(&#34;#SBATCH --gres gpu:1\n#SBATCH --gres-flags enforce-binding&#34;),
            )
        else:
            text = text.replace(PARTITION_SPECIFICS, &#34;#SBATCH --exclusive&#34;)

        # ===== Save the script =====
        script_file = &#34;sbatch.sh&#34;
        with open(os.path.join(self.project_path, script_file), &#34;w&#34;) as f:
            f.write(text)

        return script_file, job_name

    def __launch_job(self, script_file: str = None, job_name: str = None):
        &#34;&#34;&#34;Launch the job

        Args:
            script_file (str, optional): Name of the script file. Defaults to None.
            job_name (str, optional): Name of the job. Defaults to None.
        &#34;&#34;&#34;
        # ===== Submit the job =====
        print(&#34;Start to submit job!&#34;)
        subprocess.Popen([&#34;sbatch&#34;, os.path.join(self.project_path, script_file)])
        print(
            &#34;Job submitted! Script file is at: &lt;{}&gt;. Log file is at: &lt;{}&gt;&#34;.format(
                os.path.join(self.project_path, script_file),
                os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name)),
            )
        )

        # Wait for log file to be created
        current_queue = None
        queue_log_file = os.path.join(
            self.project_path, &#34;{}_queue.log&#34;.format(job_name)
        )
        with open(queue_log_file, &#34;w&#34;) as f:
            f.write(&#34;&#34;)
        print(
            &#34;Start to monitor the queue... You can check the queue at: &lt;{}&gt;&#34;.format(
                queue_log_file
            )
        )
        subprocess.Popen(
            [&#34;tail&#34;, &#34;-f&#34;, os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name))]
        )
        while True:
            time.sleep(0.25)
            if os.path.exists(
                os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name))
            ):
                break
            else:
                # Get result from squeue -p {{PARTITION_NAME}}
                result = subprocess.run(
                    [&#34;squeue&#34;, &#34;-p&#34;, &#34;gpu&#34; if self.use_gpu is True else &#34;cpu&#34;],
                    capture_output=True,
                )
                df = result.stdout.decode(&#34;utf-8&#34;).split(&#34;\n&#34;)
                users = list(
                    map(
                        lambda row: row[: len(df[0].split(&#34;ST&#34;)[0])][:-1].split(&#34; &#34;)[
                            -1
                        ],
                        df,
                    )
                )
                status = list(
                    map(
                        lambda row: row[len(df[0].split(&#34;ST&#34;)[0]) :]
                        .strip()
                        .split(&#34; &#34;)[0],
                        df,
                    )
                )
                nodes = list(
                    map(
                        lambda row: row[len(df[0].split(&#34;NODE&#34;)[0]) :]
                        .strip()
                        .split(&#34; &#34;)[0],
                        df,
                    )
                )
                node_list = list(
                    map(lambda row: row[len(df[0].split(&#34;NODELIST(REASON)&#34;)[0]) :], df)
                )

                to_queue = list(
                    zip(
                        users,
                        status,
                        nodes,
                        node_list,
                    )
                )[1:]
                if current_queue is None or current_queue != to_queue:
                    current_queue = to_queue
                    with open(queue_log_file, &#34;w&#34;) as f:
                        text = &#34;Current queue:\n&#34;
                        format_row = &#34;{:&gt;30}&#34; * (len(current_queue[0]))
                        for user, status, nodes, node_list in current_queue:
                            text += (
                                format_row.format(user, status, nodes, node_list) + &#34;\n&#34;
                            )
                        text += &#34;\n&#34;
                        f.write(text)

        # Wait for the job to finish while printing the log
        log_cursor_position = 0
        job_finished = False
        while not job_finished:
            time.sleep(0.25)
            if os.path.exists(os.path.join(self.project_path, &#34;result.pkl&#34;)):
                job_finished = True
            else:
                with open(
                    os.path.join(self.project_path, &#34;{}.log&#34;.format(job_name)), &#34;r&#34;
                ) as f:
                    f.seek(log_cursor_position)
                    text = f.read()
                    if text != &#34;&#34;:
                        print(text, end=&#34;&#34;)
                    log_cursor_position = f.tell()

        print(&#34;Job finished!&#34;)

    def __launch_server(self, cancel_old_jobs: bool = True):
        &#34;&#34;&#34;Launch the server on the cluster and run the function using the ressources.

        Args:
            cancel_old_jobs (bool, optional): Whether or not to interrupt all the user&#39;s jobs on the cluster.
        &#34;&#34;&#34;
        connected = False
        print(&#34;Connecting to the cluster...&#34;)
        ssh_client = paramiko.SSHClient()
        ssh_client.set_missing_host_key_policy(paramiko.AutoAddPolicy())
        while not connected:
            try:
                if self.server_password is None:
                    # Add ssh key
                    self.server_password = getpass(&#34;Enter your cluster password: &#34;)
                
                ssh_client.connect(
                    hostname=self.server_ssh,
                    username=self.server_username,
                    password=self.server_password,
                )
                sftp = ssh_client.open_sftp()
                connected = True
            except paramiko.ssh_exception.AuthenticationException:
                self.server_password = None
                print(&#34;Wrong password, please try again.&#34;)

        # Write server script
        self.__write_server_script()

        print(&#34;Downloading server...&#34;)
        # Generate requirements.txt
        subprocess.run(
            [f&#34;pip-chill --no-version &gt; {self.project_path}/requirements.txt&#34;],
            shell=True,
        )

        with open(f&#34;{self.project_path}/requirements.txt&#34;, &#34;r&#34;) as file:
            lines = file.readlines()
            # Adapt dependencies for the cluster
            # Adapt torch version (not needed anymore)
            # lines = [re.sub(r&#34;torch\n&#34;, &#34;torch==2.0.1\n&#34;, line) for line in lines]
            # lines = [re.sub(r&#39;torchvision\n&#39;, &#39;torchvision --pre --index-url https://download.pytorch.org/whl/nightly/cu121\n&#39;, line) for line in lines]
            # lines = [re.sub(r&#39;torchaudio\n&#39;, &#39;torchaudio --pre --index-url https://download.pytorch.org/whl/nightly/cu121\n&#39;, line) for line in lines]

            # lines = [re.sub(r&#39;bitsandbytes\n&#39;, &#39;bitsandbytes --global-option=&#34;--cuda_ext&#34;\n&#39;, line) for line in lines]
            lines = [re.sub(r&#34;slurmray\n&#34;, &#34;&#34;, line) for line in lines]
            # Add slurmray --pre
            lines.append(&#34;slurmray --pre\n&#34;)
            # Solve torch buf (https://github.com/pytorch/pytorch/issues/111469)
            if &#34;torchaudio\n&#34; or &#34;torchvision\n&#34; in lines:
                lines.append(&#34;torch==2.1.1\n&#34;)
                lines.append(&#34;--index-url https://download.pytorch.org/whl/cu121\n&#34;)

        with open(f&#34;{self.project_path}/requirements.txt&#34;, &#34;w&#34;) as file:
            file.writelines(lines)

        # Copy files from the project to the server
        for file in os.listdir(self.project_path):
            if file.endswith(&#34;.py&#34;) or file.endswith(&#34;.pkl&#34;) or file.endswith(&#34;.sh&#34;):
                sftp.put(os.path.join(self.project_path, file), file)

        # Create the server directory and remove old files
        ssh_client.exec_command(
            &#34;mkdir -p slurmray-server/.slogs/server &amp;&amp; rm -rf slurmray-server/.slogs/server/*&#34;
        )
        # Copy user files to the server
        for file in self.files:
            self.__push_file(file, sftp, ssh_client)
        # Copy the requirements.txt to the server
        sftp.put(
            os.path.join(self.project_path, &#34;requirements.txt&#34;), &#34;requirements.txt&#34;
        )
        # Copy the server script to the server
        sftp.put(
            os.path.join(self.module_path, &#34;assets&#34;, &#34;slurmray_server.sh&#34;),
            &#34;slurmray_server.sh&#34;,
        )
        # Chmod script
        sftp.chmod(&#34;slurmray_server.sh&#34;, 0o755)

        # Run the server
        print(&#34;Running server...&#34;)
        stdin, stdout, stderr = ssh_client.exec_command(&#34;./slurmray_server.sh&#34;)

        # Read the output in real time
        while True:
            line = stdout.readline()
            if not line:
                break
            print(line, end=&#34;&#34;)

        # Downloading result
        print(&#34;Downloading result...&#34;)
        sftp.get(
            &#34;slurmray-server/.slogs/server/result.pkl&#34;,
            os.path.join(self.project_path, &#34;result.pkl&#34;),
        )
        print(&#34;Result downloaded!&#34;)

    def __write_server_script(self):
        &#34;&#34;&#34;This funtion will write a script with the given specifications to run slurmray on the cluster&#34;&#34;&#34;
        print(&#34;Writing slurmray server script...&#34;)
        template_file = os.path.join(
            self.module_path, &#34;assets&#34;, &#34;slurmray_server_template.py&#34;
        )

        MODULES = self.modules
        NODE_NBR = self.node_nbr
        USE_GPU = self.use_gpu
        MEMORY = self.memory
        MAX_RUNNING_TIME = self.max_running_time

        # ===== Modified the template script =====
        with open(template_file, &#34;r&#34;) as f:
            text = f.read()
        text = text.replace(&#34;{{MODULES}}&#34;, str(MODULES))
        text = text.replace(&#34;{{NODE_NBR}}&#34;, str(NODE_NBR))
        text = text.replace(&#34;{{USE_GPU}}&#34;, str(USE_GPU))
        text = text.replace(&#34;{{MEMORY}}&#34;, str(MEMORY))
        text = text.replace(&#34;{{MAX_RUNNING_TIME}}&#34;, str(MAX_RUNNING_TIME))

        # ===== Save the script =====
        script_file = &#34;slurmray_server.py&#34;
        with open(os.path.join(self.project_path, script_file), &#34;w&#34;) as f:
            f.write(text)</code></pre>
</details>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="RayLauncher.RayLauncher" href="#RayLauncher.RayLauncher">RayLauncher</a></code></h4>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc" title="pdoc: Python API documentation generator"><cite>pdoc</cite> 0.10.0</a>.</p>
</footer>
</body>
</html>