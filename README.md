# SLURM_RAY

**Official tool from DESI @ HEC UNIL**

üëâ[Full documentation](https://www.henri-jamet.com/docs/slurmray/slurm-ray/)

## Description

**SlurmRay** is a module for effortlessly distributing tasks on a [Slurm](https://slurm.schedmd.com/) cluster (like Curnagl) or a standalone server (like ISIPOL09/Desi) using the [Ray](https://ray.io/) library. **SlurmRay** was initially designed to work with the [Curnagl](https://wiki.unil.ch/ci/books/high-performance-computing-hpc/page/curnagl) cluster at the *University of Lausanne*. It is now an official tool of the **DESI department @ HEC UNIL** and supports both Slurm-based clusters and direct SSH execution on dedicated servers.

## Installation

**SlurmRay** is designed to run both locally and on a cluster without any modification. This design is intended to allow work to be carried out on a local machine until the script seems to be working. It should then be possible to run it using all the resources of the cluster without having to modify the code.

```bash
pip install slurmray
```

## Prerequisites

### For Slurm clusters (e.g., Curnagl)
- Access to a Slurm cluster with SSH access
- Valid credentials (username/password)
- Python 3.12+ on both local and cluster machines

### For Desi server (ISIPOL09)
- VPN access to the DESI network (if required)
- SSH access to `130.223.73.209`
- Valid credentials (username/password)
- Python 3.12+ on both local and remote machines

## Principaux r√©sultats

| M√©trique | Valeur | Notes |
|---|---|---|
| Support Backend | Slurm, Desi (SSH) | Curnagl & ISIPOL09 support√©s |
| Gestion de t√¢ches | Ray | Distribution automatique |
| Dashboard | Int√©gr√© | Tunnel SSH automatique vers port 8888 |
| Compatibilit√© | Python 3.8 - 3.12 | Gestion automatique de la s√©rialisation inter-versions |

## Plan du repo

```
root/
‚îú‚îÄ‚îÄ slurmray/               # Code source du package
‚îÇ   ‚îú‚îÄ‚îÄ backend/            # Impl√©mentations backends (Slurm, Desi, Local)
‚îÇ   ‚îú‚îÄ‚îÄ assets/             # Templates de scripts (sbatch, spython)
‚îÇ   ‚îî‚îÄ‚îÄ RayLauncher.py      # Classe principale
‚îú‚îÄ‚îÄ tests/                  # Tests unitaires et d'int√©gration
‚îú‚îÄ‚îÄ documentation/          # Documentation g√©n√©r√©e
‚îú‚îÄ‚îÄ logs/                   # Logs d'ex√©cution
‚îú‚îÄ‚îÄ poetry.lock             # D√©pendances lock
‚îú‚îÄ‚îÄ pyproject.toml          # Configuration Poetry
‚îî‚îÄ‚îÄ README.md               # Documentation principale
```

## Scripts d'entr√©e principaux (scripts/)

| Chemin | Description | Exemple | Explication |
|---|---|---|---|
| `slurmray/cli.py` | Interface CLI principale | `slurmray --help` | *Point d'entr√©e pour les commandes CLI futures* |

## Scripts ex√©cutables secondaires (scripts/utils/)

| Chemin | Description | Exemple | Explication |
|---|---|---|---|
| `tests/manual_test_desi_gpu_dashboard.py` | Test manuel complet pour Desi | `python tests/manual_test_desi_gpu_dashboard.py` | *V√©rifie la connexion, le GPU, Ray et le Dashboard sur Desi* |

## Usage

### Mode 1: Slurm Cluster (Curnagl)

```python
from slurmray.RayLauncher import RayLauncher
import ray
import torch

def example_func(x):
    result = (
        ray.cluster_resources(),
        f"GPU is available : {torch.cuda.is_available()}",
        x + 1,
    )
    return result

launcher = RayLauncher(
    project_name="example_slurm",
    func=example_func,
    args={"x": 1},
    files=[],  # List of files to push to the cluster
    modules=[],  # List of modules to load (CUDA & CUDNN auto-added if use_gpu=True)
    node_nbr=1,  # Number of nodes to use
    use_gpu=True,  # Request GPU resources
    memory=8,  # RAM per node in GB
    max_running_time=5,  # Maximum runtime in minutes
    runtime_env={"env_vars": {"NCCL_SOCKET_IFNAME": "eno1"}},
    server_run=True,  # Run on cluster, not locally
    server_ssh="curnagl.dcsr.unil.ch",  # Slurm cluster address
    server_username="your_username",
    server_password=None,  # Will be prompted or loaded from .env
    cluster="slurm",  # Use Slurm backend (default)
)

# Note: When running with server_run=True, SlurmRay automatically sets up an SSH tunnel 
# to the Ray Dashboard, accessible at http://localhost:8888 during job execution.

result = launcher()
print(result)
```

### Mode 2: Desi Server (ISIPOL09)

```python
from slurmray.RayLauncher import RayLauncher
import ray

def example_func(x):
    result = (
        ray.cluster_resources(),
        x * 2,
    )
    return result

launcher = RayLauncher(
    project_name="example_desi",
    func=example_func,
    args={"x": 21},
    files=[],  # List of files to push to the server
    node_nbr=1,  # Always 1 for Desi (single server)
    use_gpu=False,  # GPU available via Smart Lock
    memory=8,  # Not enforced, shared resource
    max_running_time=30,  # Not enforced by scheduler
    server_run=True,  # Run on remote server
    server_ssh="130.223.73.209",  # Desi server IP (or use default)
    server_username="your_username",
    server_password=None,  # Will be prompted or loaded from DESI_PASSWORD env var
    cluster="desi",  # Use Desi backend (Smart Lock scheduling)
)

result = launcher()
print(result)
```

### Environment Variables

You can store credentials in a `.env` file to avoid entering them each time:

```bash
# For Curnagl
CURNAGL_USERNAME=your_username
CURNAGL_PASSWORD=your_password

# For Desi
DESI_PASSWORD=your_password
```

**Note:** The `.env` file should be in your `.gitignore` to avoid committing credentials.

## Key Differences Between Modes

| Feature | Slurm Mode | Desi Mode |
|---|---|---|
| **Scheduler** | Slurm (sbatch/squeue) | Smart Lock (file-based) |
| **Multi-node** | Supported (`node_nbr > 1`) | Single node only |
| **Modules** | Supported (`module load`) | Not supported |
| **Memory allocation** | Enforced by Slurm | Shared resource |
| **Time limit** | Enforced by Slurm | Not enforced |
| **Queue management** | Slurm queue | Smart Lock queue |
| **Default server** | `curnagl.dcsr.unil.ch` | `130.223.73.209` |

## Tests

The project includes simple "hello world" tests to quickly validate that SLURM_RAY works correctly after major modifications. These tests can be executed directly or via pytest.

### Running tests directly

```bash
# Test CPU
poetry run python tests/test_hello_world_cpu.py

# Test GPU
poetry run python tests/test_hello_world_gpu.py
```

### Running tests with pytest

```bash
# Run all tests
poetry run pytest tests/

# Run specific test
poetry run pytest tests/test_hello_world_cpu.py
poetry run pytest tests/test_hello_world_gpu.py
```

The tests require credentials for the cluster. You can provide them via a `.env` file with `CURNAGL_USERNAME` and `CURNAGL_PASSWORD`, or they will be prompted interactively.

## Publishing to PyPI

This project uses [Poetry](https://python-poetry.org/) for package management and publishing. Follow these steps to publish a new version to PyPI:

### 1. Update the version

Increment the version in `pyproject.toml` according to the type of change:

```bash
# Automatic version bumping
poetry version patch   # 3.6.4 -> 3.6.5 (bugfix)
poetry version minor   # 3.6.4 -> 3.7.0 (new feature)
poetry version major   # 3.6.4 -> 4.0.0 (breaking change)
```

Or manually edit the `version` field in `pyproject.toml`.

### 2. Build the package

```bash
poetry build
```

This creates distribution files in the `dist/` directory:
- `slurmray-{version}.tar.gz` (source distribution)
- `slurmray-{version}-py3-none-any.whl` (wheel)

### 3. Configure PyPI credentials

**First-time setup:**

1. Create an API token on [PyPI](https://pypi.org/manage/account/token/)
2. Configure Poetry to use the token:

```bash
poetry config pypi-token.pypi your-token-here
```

**Alternative:** Poetry will prompt for credentials during publishing. Use `__token__` as username and your API token as password.

### 4. Publish to PyPI

**Production (PyPI):**

```bash
poetry publish
```

**Testing (TestPyPI):**

To test the publishing process without affecting production:

```bash
poetry publish --repository testpypi
```

### Pre-publication checklist

Before publishing, ensure:

- [ ] Version incremented in `pyproject.toml`
- [ ] All tests pass (`poetry run pytest tests/`)
- [ ] README.md is up to date
- [ ] Code tested locally
- [ ] `poetry build` completes without errors
- [ ] PyPI credentials configured

### Quick reference

```bash
# Complete publishing workflow
poetry version patch          # Update version
poetry build                  # Build package
poetry publish                # Publish to PyPI

# Optional: test on TestPyPI first
poetry publish --repository testpypi
```

**Important notes:**

- Each version must be unique on PyPI (versions cannot be overwritten)
- TestPyPI is useful for testing the publishing process
- Consider creating a Git tag after publishing:
  ```bash
  git tag v3.6.5
  git push origin v3.6.5
  ```

## Launcher documentation

The Launcher documentation is available [here](https://htmlpreview.github.io/?https://raw.githubusercontent.com/hjamet/SLURM_RAY/main/documentation/RayLauncher.html).

# Roadmap

| T√¢che | Objectif | √âtat | D√©pendances |
|---|---|---|---|
| **Simplifier Affichage Queue SLURM** | Remplacer l'affichage verbeux et polluant de la file d'attente actuel par un message de statut synth√©tique et apais√© : 'Waiting for job... (Position in queue : x/X)'. Ce message ne doit √™tre rafra√Æchi que toutes les 30 secondes pour √©viter de spammer la console et les logs, am√©liorant ainsi l'exp√©rience utilisateur (UX) durant les phases d'attente. | üèóÔ∏è En cours | - |
| **Int√©grer l'ouverture automatique du dashboard Ray** | Int√©grer l'ouverture automatique du dashboard Ray dans l'interface interactive de gestion des jobs SLURM cr√©√©e pr√©c√©demment. Cette fonctionnalit√© doit permettre d'ouvrir le dashboard en local (http://localhost:8888) avec gestion automatique du port forwarding SSH si n√©cessaire. L'utilisateur doit pouvoir s√©lectionner un job en cours d'ex√©cution depuis l'interface CLI et avoir le dashboard qui s'ouvre automatiquement dans son navigateur, avec le tunnel SSH √©tabli en arri√®re-plan. Cela simplifie grandement l'acc√®s aux m√©triques de performance pour l'utilisateur final. | üèóÔ∏è En cours | Interface Interactive Jobs SLURM |
| **Consolider le transfert de code source pour la compatibilit√© Python** | G√©n√©raliser et nettoyer le m√©canisme de transfert de code source (actuellement impl√©ment√© via `inspect.getsource` pour Desi) au lieu du bytecode `dill` pour garantir la compatibilit√© entre des versions Python locales (ex: 3.12) et distantes (ex: 3.8) sur tous les backends. Cela implique de tester les limites de `inspect`, d'envisager des alternatives comme `dill.source`, et de rendre ce m√©canisme robuste pour toutes les fonctions utilisateur. | üìÖ √Ä faire | - |
| **Corriger les incompatibilit√©s avec Curnagl** | Analyser et corriger les incompatibilit√©s potentielles entre le code actuel (optimis√© pour Desi/Local) et l'environnement Curnagl (versions Python, modules SLURM, partitions). V√©rifier que les modifications r√©centes n'ont pas cass√© le support Curnagl et adapter le `RayLauncher` si n√©cessaire pour assurer une compatibilit√© parfaite avec le cluster de l'UNIL. | üìÖ √Ä faire | - |
| **Optimiser la gestion du stockage et le nettoyage des fichiers** | Optimiser la gestion du stockage et du nettoyage pour am√©liorer les performances globales du syst√®me. Impl√©menter un cache intelligent pour r√©utiliser le virtualenv entre ex√©cutions si les d√©pendances n'ont pas chang√©, √©vitant ainsi de recr√©er l'environnement √† chaque fois. Nettoyer syst√©matiquement les fichiers temporaires apr√®s t√©l√©chargement r√©ussi des r√©sultats pour √©viter l'accumulation de donn√©es inutiles. Optimiser la g√©n√©ration de `requirements.txt` pour qu'elle soit plus rapide et plus pr√©cise. Corriger les incoh√©rences potentielles de versions Python entre l'environnement local et distant pour garantir la compatibilit√©. | üìÖ √Ä faire | - |
| **Interface Interactive Jobs SLURM** | D√©velopper une interface en ligne de commande (TUI simple ou menu interactif) accessible via `python -m slurmray`. Cette interface permettra aux utilisateurs de lister leurs jobs en cours, de voir leur position pr√©cise dans la file d'attente, et de les annuler facilement sans avoir √† m√©moriser les commandes `scancel` ou `squeue` complexes. | üìÖ √Ä faire | Simplifier Affichage Queue SLURM |
| **Int√©gration Point d'Entr√©e** | Finaliser l'impl√©mentation du fichier `__main__.py` dans le package pour exposer proprement l'interface interactive cr√©√©e pr√©c√©demment. S'assurer que la commande `python -m slurmray` est intuitive et g√®re correctement les exceptions (ex: absence de credentials). | üìÖ √Ä faire | Interface Interactive Jobs SLURM |
| **Am√©liorer la gestion des credentials (username/password) via .env** | Modifier RayLauncher pour charger automatiquement `server_username` et `server_password` depuis un fichier `.env` local, tout en gardant la r√©trocompatibilit√© avec les param√®tres explicites pass√©s au constructeur. Le syst√®me doit d'abord v√©rifier les variables d'environnement (via `python-dotenv`), puis les param√®tres explicites, et enfin demander interactivement si aucun n'est trouv√©. Cette am√©lioration am√©liore la s√©curit√© (√©vite de hardcoder les mots de passe) et l'ergonomie pour les utilisateurs fr√©quents qui peuvent stocker leurs credentials de mani√®re s√©curis√©e dans un fichier `.env` ignor√© par Git. | üìÖ √Ä faire | - |
| **Mettre √† jour la documentation pour tout avoir dans le repo** | Remplacer les liens externes dans README.md par du contenu local, int√©grer la documentation de RayLauncher directement dans le repository pour √©viter les d√©pendances vers des sites externes. Migrer toute la documentation externe (liens actuels vers sites tiers ou HTML pr√©visualis√©s) directement dans le d√©p√¥t (dossier `docs/` ou Markdown). L'objectif est que le repository soit auto-suffisant et que la documentation versionn√©e suive l'√©volution du code. Cela garantit que la documentation est toujours √† jour et accessible m√™me si les sites externes changent ou disparaissent. | üìÖ √Ä faire | - |
