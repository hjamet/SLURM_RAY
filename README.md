# SLURM_RAY

**Official tool from DESI @ HEC UNIL**

üëâ[Full documentation](https://www.henri-jamet.com/docs/slurmray/slurm-ray/)

## Description

**SlurmRay** is a module for effortlessly distributing tasks on a [Slurm](https://slurm.schedmd.com/) cluster (like Curnagl) or a standalone server (like ISIPOL09/Desi) using the [Ray](https://ray.io/) library. **SlurmRay** was initially designed to work with the [Curnagl](https://wiki.unil.ch/ci/books/high-performance-computing-hpc/page/curnagl) cluster at the *University of Lausanne*. It is now an official tool of the **DESI department @ HEC UNIL** and supports both Slurm-based clusters and direct SSH execution on dedicated servers.

## Installation

**SlurmRay** is designed to run both locally and on a cluster without any modification. This design is intended to allow work to be carried out on a local machine until the script seems to be working. It should then be possible to run it using all the resources of the cluster without having to modify the code.

```bash
pip install slurmray
```

## Prerequisites

### For Slurm clusters (e.g., Curnagl)
- Access to a Slurm cluster with SSH access
- Valid credentials (username/password)
- Python 3.12+ on both local and cluster machines

### For Desi server (ISIPOL09)
- VPN access to the DESI network (if required)
- SSH access to `130.223.73.209`
- Valid credentials (username/password)
- Python 3.12+ on both local and remote machines

## Principaux r√©sultats

| M√©trique | Valeur | Notes |
|---|---|---|
| Support Backend | Slurm, Desi (SSH) | Curnagl & ISIPOL09 support√©s |
| Gestion de t√¢ches | Ray | Distribution automatique |
| Installation | Optimis√©e | Installation incr√©mentale avec cache et d√©tection de versions |
| Dashboard | Int√©gr√© | Ouverture automatique dans le navigateur (via tunnel SSH) |
| Compatibilit√© | Python 3.8 - 3.12 | Gestion automatique de la s√©rialisation inter-versions |

## Plan du repo

```
root/
‚îú‚îÄ‚îÄ slurmray/               # Code source du package
‚îÇ   ‚îú‚îÄ‚îÄ backend/            # Impl√©mentations backends (Slurm, Desi, Local)
‚îÇ   ‚îú‚îÄ‚îÄ assets/             # Templates de scripts (sbatch, spython)
‚îÇ   ‚îî‚îÄ‚îÄ RayLauncher.py      # Classe principale
‚îú‚îÄ‚îÄ tests/                  # Tests unitaires et d'int√©gration
‚îú‚îÄ‚îÄ documentation/          # Documentation g√©n√©r√©e
‚îú‚îÄ‚îÄ logs/                   # Logs d'ex√©cution
‚îú‚îÄ‚îÄ poetry.lock             # D√©pendances lock
‚îú‚îÄ‚îÄ pyproject.toml          # Configuration Poetry
‚îî‚îÄ‚îÄ README.md               # Documentation principale
```

## Scripts d'entr√©e principaux (scripts/)

| Chemin | Description | Exemple | Explication |
|---|---|---|---|
| `slurmray/cli.py` | Interface CLI principale | `slurmray curnagl` ou `slurmray desi` | *Lance l'interface interactive pour g√©rer les jobs et acc√©der au dashboard. Supporte Curnagl (Slurm) et Desi (ISIPOL09). Par d√©faut, affiche l'aide si aucun cluster n'est sp√©cifi√©.* |

## Scripts ex√©cutables secondaires (scripts/utils/)

| Chemin | Description | Exemple | Explication |
|---|---|---|---|
| `tests/test_gpu_dashboard_long.py` | Test GPU et dashboard avec job long | `poetry run python tests/test_gpu_dashboard_long.py` | *Lance un job GPU de 5 minutes pour tester le dashboard via l'interface CLI* |
| `tests/manual_test_desi_gpu_dashboard.py` | Test manuel complet pour Desi | `python tests/manual_test_desi_gpu_dashboard.py` | *V√©rifie la connexion, le GPU, Ray et le Dashboard sur Desi* |

## Usage

### Mode 1: Slurm Cluster (Curnagl)

```python
from slurmray.RayLauncher import RayLauncher
import ray
import torch

def example_func(x):
    result = (
        ray.cluster_resources(),
        f"GPU is available : {torch.cuda.is_available()}",
        x + 1,
    )
    return result

launcher = RayLauncher(
    project_name="example_slurm",
    func=example_func,
    args={"x": 1},
    files=[],  # List of files to push to the cluster
    modules=[],  # List of modules to load (CUDA & CUDNN auto-added if use_gpu=True)
    node_nbr=1,  # Number of nodes to use
    use_gpu=True,  # Request GPU resources
    memory=8,  # RAM per node in GB
    max_running_time=5,  # Maximum runtime in minutes
    runtime_env={"env_vars": {"NCCL_SOCKET_IFNAME": "eno1"}},
    server_run=True,  # Run on cluster, not locally
    server_ssh="curnagl.dcsr.unil.ch",  # Slurm cluster address
    server_username="your_username",
    server_password=None,  # Will be prompted or loaded from .env
    cluster="slurm",  # Use Slurm backend (default)
)

# Note: When running with server_run=True, SlurmRay automatically sets up an SSH tunnel 
# to the Ray Dashboard, accessible at http://localhost:8888 during job execution.

result = launcher()
print(result)
```

### Mode 2: Desi Server (ISIPOL09)

```python
from slurmray.RayLauncher import RayLauncher
import ray

def example_func(x):
    result = (
        ray.cluster_resources(),
        x * 2,
    )
    return result

launcher = RayLauncher(
    project_name="example_desi",
    func=example_func,
    args={"x": 21},
    files=[],  # List of files to push to the server
    node_nbr=1,  # Always 1 for Desi (single server)
    use_gpu=False,  # GPU available via Smart Lock
    memory=8,  # Not enforced, shared resource
    max_running_time=30,  # Not enforced by scheduler
    server_run=True,  # Run on remote server
    server_ssh="130.223.73.209",  # Desi server IP (or use default)
    server_username="your_username",  # Will be loaded from DESI_USERNAME env var if not provided
    server_password=None,  # Will be prompted or loaded from DESI_PASSWORD env var
    cluster="desi",  # Use Desi backend (Smart Lock scheduling)
)

result = launcher()
print(result)
```

### Environment Variables

You can store credentials in a `.env` file to avoid entering them each time:

```bash
# For Curnagl
CURNAGL_USERNAME=your_username
CURNAGL_PASSWORD=your_password

# For Desi
DESI_USERNAME=your_username
DESI_PASSWORD=your_password
```

**Note:** The `.env` file should be in your `.gitignore` to avoid committing credentials.

## Key Differences Between Modes

| Feature | Slurm Mode | Desi Mode |
|---|---|---|
| **Scheduler** | Slurm (sbatch/squeue) | Smart Lock (file-based) |
| **Multi-node** | Supported (`node_nbr > 1`) | Single node only |
| **Modules** | Supported (`module load`) | Not supported |
| **Memory allocation** | Enforced by Slurm | Shared resource |
| **Time limit** | Enforced by Slurm | Not enforced |
| **Queue management** | Slurm queue | Smart Lock queue |
| **Default server** | `curnagl.dcsr.unil.ch` | `130.223.73.209` |

## Function Serialization and Python Version Compatibility

SlurmRay uses **source code extraction** (via `inspect.getsource()` or `dill.source.getsource()`) as the primary method for function serialization. This approach provides better compatibility across Python versions (e.g., Python 3.12 locally and Python 3.8 on the remote server) compared to bytecode serialization.

### How It Works

1. **Source extraction**: The function's source code is extracted and saved to `func_source.py`
2. **Remote execution**: The source code is executed on the remote server, avoiding bytecode incompatibilities
3. **Fallback**: If source extraction fails, SlurmRay falls back to `dill` bytecode serialization (may fail with version mismatches)

### Limitations

**Functions with closures**: Only the function body is captured, not the captured variables. Functions that depend on closure variables may fail at runtime.

**Functions with global dependencies**: Global variables referenced in the function are not automatically included. Ensure all required globals are available on the remote server or pass them as function arguments.

**Built-in functions**: Built-in functions (e.g., `len`, `max`) cannot be serialized via source extraction and will fall back to `dill`.

**Dynamically created functions**: Functions created at runtime or in interactive shells may not have accessible source code.

### Best Practices

- **Prefer simple functions**: Functions with minimal dependencies work best
- **Pass dependencies as arguments**: Instead of using closures or globals, pass required values as function arguments
- **Test locally first**: Validate your function works correctly before submitting to the cluster
- **Check logs**: If source extraction fails, check the logs for warnings and ensure `func.pkl` fallback is available

## Tests

The project includes simple "hello world" tests to quickly validate that SLURM_RAY works correctly after major modifications. These tests can be executed directly or via pytest.

### Running tests directly

```bash
# Test CPU
poetry run python tests/test_hello_world_cpu.py

# Test GPU
poetry run python tests/test_hello_world_gpu.py
```

### Running tests with pytest

```bash
# Run all tests
poetry run pytest tests/

# Run specific test
poetry run pytest tests/test_hello_world_cpu.py
poetry run pytest tests/test_hello_world_gpu.py
```

The tests require credentials for the cluster. You can provide them via a `.env` file with `CURNAGL_USERNAME` and `CURNAGL_PASSWORD`, or they will be prompted interactively.

## Publishing to PyPI

This project uses [Poetry](https://python-poetry.org/) for package management and publishing. Follow these steps to publish a new version to PyPI:

### 1. Update the version

Increment the version in `pyproject.toml` according to the type of change:

```bash
# Automatic version bumping
poetry version patch   # 3.6.4 -> 3.6.5 (bugfix)
poetry version minor   # 3.6.4 -> 3.7.0 (new feature)
poetry version major   # 3.6.4 -> 4.0.0 (breaking change)
```

Or manually edit the `version` field in `pyproject.toml`.

### 2. Build the package

```bash
poetry build
```

This creates distribution files in the `dist/` directory:
- `slurmray-{version}.tar.gz` (source distribution)
- `slurmray-{version}-py3-none-any.whl` (wheel)

### 3. Configure PyPI credentials

**First-time setup:**

1. Create an API token on [PyPI](https://pypi.org/manage/account/token/)
2. Configure Poetry to use the token:

```bash
poetry config pypi-token.pypi your-token-here
```

**Alternative:** Poetry will prompt for credentials during publishing. Use `__token__` as username and your API token as password.

### 4. Publish to PyPI

**Production (PyPI):**

```bash
poetry publish
```

**Testing (TestPyPI):**

To test the publishing process without affecting production:

```bash
poetry publish --repository testpypi
```

### Pre-publication checklist

Before publishing, ensure:

- [ ] Version incremented in `pyproject.toml`
- [ ] All tests pass (`poetry run pytest tests/`)
- [ ] README.md is up to date
- [ ] Code tested locally
- [ ] `poetry build` completes without errors
- [ ] PyPI credentials configured

### Quick reference

```bash
# Complete publishing workflow
poetry version patch          # Update version
poetry build                  # Build package
poetry publish                # Publish to PyPI

# Optional: test on TestPyPI first
poetry publish --repository testpypi
```

**Important notes:**

- Each version must be unique on PyPI (versions cannot be overwritten)
- TestPyPI is useful for testing the publishing process
- Consider creating a Git tag after publishing:
  ```bash
  git tag v3.6.5
  git push origin v3.6.5
  ```

## Launcher documentation

The Launcher documentation is available [here](https://htmlpreview.github.io/?https://raw.githubusercontent.com/hjamet/SLURM_RAY/main/documentation/RayLauncher.html).

# Roadmap

| T√¢che | Objectif | √âtat | D√©pendances |
|---|---|---|---|
| **Corriger les incompatibilit√©s avec Curnagl** | Analyser et corriger les incompatibilit√©s potentielles entre le code actuel (optimis√© pour Desi/Local) et l'environnement Curnagl (versions Python, modules SLURM, partitions). V√©rifier que les modifications r√©centes n'ont pas cass√© le support Curnagl et adapter le `RayLauncher` si n√©cessaire pour assurer une compatibilit√© parfaite avec le cluster de l'UNIL. | üìÖ √Ä faire | - |
| **Optimiser la gestion du stockage et le nettoyage des fichiers** | Optimiser la gestion du stockage et du nettoyage pour am√©liorer les performances globales du syst√®me. Impl√©menter un cache intelligent pour r√©utiliser le virtualenv entre ex√©cutions si les d√©pendances n'ont pas chang√©, √©vitant ainsi de recr√©er l'environnement √† chaque fois. Nettoyer syst√©matiquement les fichiers temporaires apr√®s t√©l√©chargement r√©ussi des r√©sultats pour √©viter l'accumulation de donn√©es inutiles. Optimiser la g√©n√©ration de `requirements.txt` pour qu'elle soit plus rapide et plus pr√©cise. Corriger les incoh√©rences potentielles de versions Python entre l'environnement local et distant pour garantir la compatibilit√©. | üìÖ √Ä faire | - |
| **Ajouter la possibilit√© de forcer la r√©installation compl√®te des requirements** | Actuellement, les environnements virtuels (`.venv` pour Slurm, `venv` pour Desi) sont cr√©√©s automatiquement s'ils n'existent pas, mais il n'existe pas de m√©canisme pour forcer une r√©installation compl√®te en cas de corruption, d'incompatibilit√© de versions, ou de besoin de nettoyage. Cette t√¢che consiste √† ajouter une option (flag ou param√®tre) permettant de forcer la suppression compl√®te de l'environnement virtuel existant et sa recr√©ation depuis z√©ro, suivie d'une r√©installation compl√®te de tous les packages depuis `requirements.txt`. L'impl√©mentation doit couvrir √† la fois l'environnement local (Poetry avec `.venv`) et les environnements distants (Slurm avec `.venv` dans `slurmray-server/`, Desi avec `venv` dans le r√©pertoire de projet). Pour l'environnement local, cela pourrait √™tre un script d'installation (`install.sh` ou commande Poetry) avec un flag `--force-reinstall` ou `--clean`. Pour les environnements distants, cela pourrait √™tre un param√®tre du `RayLauncher` (ex: `force_reinstall_venv=True`) qui modifie les scripts g√©n√©r√©s (`slurmray_server.sh` pour Slurm, `runner_script.sh` pour Desi) pour supprimer le venv existant avant cr√©ation. La suppression doit √™tre s√©curis√©e (v√©rifier que le venv n'est pas utilis√© par un job en cours) et la recr√©ation doit suivre exactement le m√™me processus que la cr√©ation normale, garantissant la coh√©rence. | üìÖ √Ä faire | - |
| **Am√©liorer la gestion des credentials (username/password) via .env** | Modifier RayLauncher pour charger automatiquement `server_username` et `server_password` depuis un fichier `.env` local, tout en gardant la r√©trocompatibilit√© avec les param√®tres explicites pass√©s au constructeur. Le syst√®me doit d'abord v√©rifier les variables d'environnement (via `python-dotenv`), puis les param√®tres explicites, et enfin demander interactivement si aucun n'est trouv√©. Cette am√©lioration am√©liore la s√©curit√© (√©vite de hardcoder les mots de passe) et l'ergonomie pour les utilisateurs fr√©quents qui peuvent stocker leurs credentials de mani√®re s√©curis√©e dans un fichier `.env` ignor√© par Git. | üìÖ √Ä faire | - |
| **Mettre √† jour la documentation pour tout avoir dans le repo** | Remplacer les liens externes dans README.md par du contenu local, int√©grer la documentation de RayLauncher directement dans le repository pour √©viter les d√©pendances vers des sites externes. Migrer toute la documentation externe (liens actuels vers sites tiers ou HTML pr√©visualis√©s) directement dans le d√©p√¥t (dossier `docs/` ou Markdown). L'objectif est que le repository soit auto-suffisant et que la documentation versionn√©e suive l'√©volution du code. Cela garantit que la documentation est toujours √† jour et accessible m√™me si les sites externes changent ou disparaissent. | üìÖ √Ä faire | - |
| **Cr√©er des scripts de test GPU et dashboard pour Curnagl et Desi** | Cr√©er deux scripts de test automatis√©s et complets pour valider le bon fonctionnement des deux clusters. Le script pour Curnagl (`tests/test_curnagl_gpu_dashboard.py`) doit lancer un job Slurm avec GPU, v√©rifier l'acc√®s au GPU via PyTorch (disponibilit√© CUDA, nombre de GPUs, noms des GPUs), valider les ressources Ray, et s'assurer que le dashboard Ray est accessible localement via le tunnel SSH automatique sur http://localhost:8888 pendant l'ex√©cution du job. Le script pour Desi (`tests/test_desi_gpu_dashboard.py`) doit effectuer les m√™mes v√©rifications mais adapt√©es au backend Desi (Smart Lock, pas de modules Slurm). Les deux scripts doivent inclure des v√©rifications explicites de l'accessibilit√© du dashboard local (test de connexion HTTP sur le port local, v√©rification que le tunnel SSH est actif, validation que le contenu du dashboard r√©pond correctement). Apr√®s la cr√©ation des scripts, ex√©cuter le script Desi pour valider imm√©diatement l'acc√®s au GPU et l'accessibilit√© locale du dashboard sur le serveur ISIPOL09. Ces scripts serviront de tests de validation rapide apr√®s toute modification importante du syst√®me de lancement ou des backends. | üìÖ √Ä faire | - |
