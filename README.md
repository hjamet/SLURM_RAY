# SLURM_RAY

ğŸ‘‰[Full documentation](https://www.henri-jamet.com/docs/slurmray/slurm-ray/)

## Description

**SlurmRay** is a module for effortlessly distributing tasks on a [Slurm](https://slurm.schedmd.com/) cluster using the [Ray](https://ray.io/) library. **SlurmRay** was initially designed to work with the [Curnagl](https://wiki.unil.ch/ci/books/high-performance-computing-hpc/page/curnagl) cluster at the *University of Lausanne*. However, it should be able to run on any [Slurm](https://slurm.schedmd.com/) cluster with a minimum of configuration.

## Installation

**SlurmRay** is designed to run both locally and on a cluster without any modification. This design is intended to allow work to be carried out on a local machine until the script seems to be working. It should then be possible to run it using all the resources of the cluster without having to modify the code.

```bash
pip install slurmray
```

## Usage

```python
from slurmray.RayLauncher import RayLauncher
import ray
import torch

def function_inside_function():
    with open("slurmray/RayLauncher.py", "r") as f:
        return f.read()[0:10]

def example_func(x):
    result = (
        ray.cluster_resources(),
        f"GPU is available : {torch.cuda.is_available()}",
        x + 1,
        function_inside_function(),
    )
    return result

launcher = RayLauncher(
    project_name="example", # Name of the project (will create a directory with this name in the current directory)
    func=example_func, # Function to execute
    args={"x": 1}, # Arguments of the function
    files=["slurmray/RayLauncher.py"], # List of files to push to the cluster (file path will be recreated on the cluster)
    modules=[], # List of modules to load on the curnagl Cluster (CUDA & CUDNN are automatically added if use_gpu=True)
    node_nbr=1, # Number of nodes to use
    use_gpu=True, # If you need A100 GPU, you can set it to True
    memory=8, # In MegaBytes
    max_running_time=5, # In minutes
    runtime_env={"env_vars": {"NCCL_SOCKET_IFNAME": "eno1"}}, # Example of environment variable
    server_run=True, # To run the code on the cluster and not locally
    server_ssh="curnagl.dcsr.unil.ch", # Address of the SLURM server
    server_username="hjamet", # Username to connect to the server
    server_password=None, # Will be asked in the terminal
)

result = launcher()
print(result)
```

## Tests

The project includes simple "hello world" tests to quickly validate that SLURM_RAY works correctly after major modifications. These tests can be executed directly or via pytest.

### Running tests directly

```bash
# Test CPU
poetry run python tests/test_hello_world_cpu.py

# Test GPU
poetry run python tests/test_hello_world_gpu.py
```

### Running tests with pytest

```bash
# Run all tests
poetry run pytest tests/

# Run specific test
poetry run pytest tests/test_hello_world_cpu.py
poetry run pytest tests/test_hello_world_gpu.py
```

The tests require credentials for the cluster. You can provide them via a `.env` file with `CURNAGL_USERNAME` and `CURNAGL_PASSWORD`, or they will be prompted interactively.

## Publishing to PyPI

This project uses [Poetry](https://python-poetry.org/) for package management and publishing. Follow these steps to publish a new version to PyPI:

### 1. Update the version

Increment the version in `pyproject.toml` according to the type of change:

```bash
# Automatic version bumping
poetry version patch   # 3.6.4 -> 3.6.5 (bugfix)
poetry version minor   # 3.6.4 -> 3.7.0 (new feature)
poetry version major   # 3.6.4 -> 4.0.0 (breaking change)
```

Or manually edit the `version` field in `pyproject.toml`.

### 2. Build the package

```bash
poetry build
```

This creates distribution files in the `dist/` directory:
- `slurmray-{version}.tar.gz` (source distribution)
- `slurmray-{version}-py3-none-any.whl` (wheel)

### 3. Configure PyPI credentials

**First-time setup:**

1. Create an API token on [PyPI](https://pypi.org/manage/account/token/)
2. Configure Poetry to use the token:

```bash
poetry config pypi-token.pypi your-token-here
```

**Alternative:** Poetry will prompt for credentials during publishing. Use `__token__` as username and your API token as password.

### 4. Publish to PyPI

**Production (PyPI):**

```bash
poetry publish
```

**Testing (TestPyPI):**

To test the publishing process without affecting production:

```bash
poetry publish --repository testpypi
```

### Pre-publication checklist

Before publishing, ensure:

- [ ] Version incremented in `pyproject.toml`
- [ ] All tests pass (`poetry run pytest tests/`)
- [ ] README.md is up to date
- [ ] Code tested locally
- [ ] `poetry build` completes without errors
- [ ] PyPI credentials configured

### Quick reference

```bash
# Complete publishing workflow
poetry version patch          # Update version
poetry build                  # Build package
poetry publish                # Publish to PyPI

# Optional: test on TestPyPI first
poetry publish --repository testpypi
```

**Important notes:**

- Each version must be unique on PyPI (versions cannot be overwritten)
- TestPyPI is useful for testing the publishing process
- Consider creating a Git tag after publishing:
  ```bash
  git tag v3.6.5
  git push origin v3.6.5
  ```

## Launcher documentation

The Launcher documentation is available [here](https://htmlpreview.github.io/?https://raw.githubusercontent.com/hjamet/SLURM_RAY/main/documentation/RayLauncher.html).

# Roadmap

| TÃ¢che | Objectif | Ã‰tat | DÃ©pendances |
|-------|----------|------|-------------|
| **Corriger les incompatibilitÃ©s Curnagl** | Analyser et corriger les incompatibilitÃ©s critiques entre le code actuel et l'environnement du cluster Curnagl. Cela inclut la vÃ©rification des versions Python, l'ajustement des directives de chargement des modules (module load), la correction des arguments SLURM dÃ©prÃ©ciÃ©s, et la validation des partitions disponibles. L'objectif est de restaurer une exÃ©cution stable des jobs simples sur Curnagl avant toute Ã©volution majeure. | ğŸ—ï¸ En cours | - |
| **Corriger Dashboard Ray** | RÃ©soudre le bug de configuration Ã  la ligne 199 de `RayLauncher.py` qui empÃªche le lancement correct du dashboard. ImplÃ©menter ensuite une redirection de port automatique via le tunnel SSH existant pour rendre le dashboard Ray accessible localement sur `http://localhost:8888` pendant l'exÃ©cution du job, offrant ainsi une visibilitÃ© en temps rÃ©el Ã  l'utilisateur. | ğŸ—ï¸ En cours | - |
| **Simplifier Affichage Queue SLURM** | Remplacer l'affichage verbeux et polluant de la file d'attente actuel par un message de statut synthÃ©tique et apaisÃ© : 'Waiting for job... (Position in queue : x/X)'. Ce message ne doit Ãªtre rafraÃ®chi que toutes les 30 secondes pour Ã©viter de spammer la console et les logs, amÃ©liorant ainsi l'expÃ©rience utilisateur (UX) durant les phases d'attente. | ğŸ—ï¸ En cours | - |
| **Refactoring Architecture Strategy** | Restructurer le cÅ“ur de l'application `RayLauncher.py` en appliquant le Design Pattern "Strategy". Il s'agit de crÃ©er une classe abstraite `ClusterBackend` dÃ©finissant l'interface commune (`submit`, `status`, `cancel`, `get_logs`), puis d'encapsuler toute la logique Slurm actuelle dans une classe concrÃ¨te `SlurmBackend`. Cette Ã©tape est cruciale pour permettre l'ajout futur du backend "Desi" sans complexifier le code avec des conditions multiples. Le code existant doit Ãªtre dÃ©placÃ© sans rÃ©gression. | ğŸ“… Ã€ faire | Corriger les incompatibilitÃ©s Curnagl |
| **ImplÃ©mentation Backend Desi** | DÃ©velopper la classe `DesiBackend` implÃ©mentant `ClusterBackend` pour le serveur `isipol09`. Ce backend doit gÃ©rer une exÃ©cution "Stateless" via SSH/SFTP : crÃ©ation d'un dossier temporaire unique par job, upload du code sÃ©rialisÃ© (cloudpickle) et des dÃ©pendances, exÃ©cution d'un script "runner" distant qui lance une instance Ray isolÃ©e (ports dynamiques), rÃ©cupÃ©ration des rÃ©sultats, et suppression impÃ©rative du dossier temporaire (nettoyage fail-fast) pour ne pas polluer le serveur partagÃ©. | ğŸ“… Ã€ faire | Refactoring Architecture Strategy |
| **Unification et Arguments** | Mettre Ã  jour la classe principale `RayLauncher` (le Context du pattern Strategy) pour instancier dynamiquement le bon backend selon l'argument `cluster='curnagl'` ou `cluster='desi'`. ImplÃ©menter une validation des arguments conditionnelle : avertir si des arguments spÃ©cifiques Ã  Slurm (partitions, time_limit) sont passÃ©s en mode Desi, et adapter la gestion de la demande de GPU (`use_gpu`) pour qu'elle fonctionne correctement avec le backend Desi. | ğŸ“… Ã€ faire | ImplÃ©mentation Backend Desi |
| **Optimiser Stockage et Nettoyage** | AmÃ©liorer la performance et l'hygiÃ¨ne du projet : implÃ©menter un systÃ¨me de cache intelligent pour Ã©viter de recrÃ©er le virtualenv Ã  chaque exÃ©cution si les dÃ©pendances n'ont pas changÃ©, optimiser la gÃ©nÃ©ration du `requirements.txt`, et garantir un nettoyage systÃ©matique des fichiers temporaires locaux aprÃ¨s le tÃ©lÃ©chargement des rÃ©sultats. Corriger Ã©galement les incohÃ©rences potentielles de versions Python entre le local et le distant. | ğŸ“… Ã€ faire | Corriger les incompatibilitÃ©s Curnagl |
| **Rebranding et Documentation** | Mettre Ã  jour l'identitÃ© du projet pour reflÃ©ter son nouveau statut d'outil officiel du dÃ©partement DESI @ HEC UNIL. Actualiser le README, les docstrings et les mÃ©tadonnÃ©es PyPI pour documenter clairement les deux modes d'exÃ©cution (Curnagl/Slurm et Desi/SSH), les prÃ©-requis respectifs, et fournir des exemples d'utilisation adaptÃ©s aux nouveaux utilisateurs du dÃ©partement. | ğŸ“… Ã€ faire | Unification et Arguments |
| **Tester Package CPU/GPU** | CrÃ©er une suite de tests fonctionnels robustes (automatisables via CI si possible, ou scriptÃ©s) qui lancent de vÃ©ritables petits jobs "Hello World" sur CPU et GPU. Ces tests serviront de "Smoke Tests" Ã  lancer avant chaque release pour garantir que la chaÃ®ne complÃ¨te (soumission -> exÃ©cution -> rÃ©cupÃ©ration) fonctionne sur les deux environnements cibles. | ğŸ“… Ã€ faire | Refactoring Architecture Strategy, Optimiser Stockage et Nettoyage |
| **Interface Interactive Jobs SLURM** | DÃ©velopper une interface en ligne de commande (TUI simple ou menu interactif) accessible via `python -m slurmray`. Cette interface permettra aux utilisateurs de lister leurs jobs en cours, de voir leur position prÃ©cise dans la file d'attente, et de les annuler facilement sans avoir Ã  mÃ©moriser les commandes `scancel` ou `squeue` complexes. | ğŸ“… Ã€ faire | Simplifier Affichage Queue SLURM |
| **IntÃ©gration Point d'EntrÃ©e** | Finaliser l'implÃ©mentation du fichier `__main__.py` dans le package pour exposer proprement l'interface interactive crÃ©Ã©e prÃ©cÃ©demment. S'assurer que la commande `python -m slurmray` est intuitive et gÃ¨re correctement les exceptions (ex: absence de credentials). | ğŸ“… Ã€ faire | Interface Interactive Jobs SLURM |
| **Ouverture Auto Dashboard** | IntÃ©grer dans l'interface interactive et le launcher une fonctionnalitÃ© d'ouverture automatique du navigateur vers le dashboard Ray local (aprÃ¨s Ã©tablissement du tunnel SSH). Cela simplifie l'accÃ¨s aux mÃ©triques de performance pour l'utilisateur final. | ğŸ“… Ã€ faire | Interface Interactive Jobs SLURM, Corriger Dashboard Ray |
| **AmÃ©liorer Credentials .env** | Moderniser la gestion des identifiants en supportant nativement le chargement depuis un fichier `.env` local. Le launcher doit chercher `SLURM_USERNAME`/`SLURM_PASSWORD` (ou Ã©quivalents) dans l'environnement avant de demander Ã  l'utilisateur ou d'utiliser les arguments, amÃ©liorant ainsi la sÃ©curitÃ© et l'ergonomie pour les utilisateurs frÃ©quents. | ğŸ“… Ã€ faire | - |
| **Documentation Interne** | Migrer toute la documentation externe (liens actuels vers sites tiers ou HTML prÃ©visualisÃ©s) directement dans le dÃ©pÃ´t (dossier `docs/` ou Markdown). L'objectif est que le repository soit auto-suffisant et que la documentation versionnÃ©e suive l'Ã©volution du code. | ğŸ“… Ã€ faire | Rebranding et Documentation |
